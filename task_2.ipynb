{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcfNBWaxRB84"
      },
      "source": [
        "2. Using image features, such as colour, histogram-of-gradients (e.g. SIFT/SURF), and/or\n",
        "texture or Gabor features, write a feature based matcher to compare pairs of card images\n",
        "and determine if they are similar. Using your image matcher, on all the 'unidentified' cards\n",
        "in train-001.jpg to find the closest M matches, e.g. M = 5, from all the other cards in the\n",
        "training images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE HAS BEEN TESTED ON GOOGLE COLAB ON PYTHON VERSION 3.7.12"
      ],
      "metadata": {
        "id": "wFJLG3iMG9Kb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76cIk_vJnxOc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLe2cdoiIC_K"
      },
      "source": [
        "**Importing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZBrqlycn1FT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt                                                   #for plotting images \n",
        "import numpy as np                                                                #library for mathematical operations            \n",
        "from PIL import Image                                                             \n",
        "import glob                                                                       #for reading files\n",
        "import os                                                                         #for performing os operations\n",
        "                    \n",
        "import matplotlib.image as mpimg                                                  \n",
        "import cv2                                                                        #opencv\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jENX8SY3Gt8S"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44                                      #downgrade opencv to use sift_create "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLPe1pOan4Es"
      },
      "outputs": [],
      "source": [
        "data_dir='drive/My Drive/Image_Analysis/DATA/'                                    #data directory path\n",
        "results_dir='drive/My Drive/Image_Analysis/RESULTS_3/'                            #results directory path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywddF_TCIKEj"
      },
      "source": [
        "**Reading card images from results_dir which were output of task-1** : Separated cards which were output of task-1 have been imported using glob from the results_dir. Cards with corresponding training set have been put into same class. E.g. cards from train_001 have been assigned class=1 and train_002 have been assigned class=2 and so on. \n",
        "\n",
        "All card images have been appended to images[] with corresponding classes in classes[]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GDpTkHOId2c"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "classes = []\n",
        "names=[]\n",
        "\n",
        "for i in range(10):\n",
        "  \n",
        "  filenames = glob.glob(results_dir + 'train-00'+ str(i+1) +'*.jpg')              #reading training sets from 001-009\n",
        "\n",
        "  for j in range(len(filenames)):      \n",
        "    im_gbr = cv2.imread(filenames[j])\n",
        "    name = os.path.splitext(os.path.basename(filenames[j]))[0] \n",
        "    im = cv2.cvtColor(im_gbr,cv2.COLOR_BGR2RGB)                                   # convert from BGR ordering to RGB\n",
        "\n",
        "    images.append(im)\n",
        "    names.append(name)\n",
        "    classes.append(i+1)\n",
        "   \n",
        "\n",
        "filenames = glob.glob(results_dir + 'train-010'+'*.jpg')\n",
        "\n",
        "for j in range(len(filenames)):      \n",
        "    \n",
        "    im = cv2.imread(filenames[j])\n",
        "    name = os.path.splitext(os.path.basename(filenames[j]))[0]\n",
        "    im = cv2.cvtColor(im_gbr,cv2.COLOR_BGR2RGB)                                    # convert from BGR ordering to RGB\n",
        "\n",
        "    images.append(im)\n",
        "    names.append(name)\n",
        "    classes.append(10)\n",
        "   \n",
        "\n",
        "filenames = glob.glob(results_dir + 'train-012'+'*.jpg')\n",
        "\n",
        "for j in range(len(filenames)):      \n",
        "   \n",
        "    im = cv2.imread(filenames[j])\n",
        "    name = os.path.splitext(os.path.basename(filenames[j]))[0]\n",
        "    im = cv2.cvtColor(im_gbr,cv2.COLOR_BGR2RGB)                                   # convert from BGR ordering to RGB\n",
        "\n",
        "    images.append(im)\n",
        "    names.append(name)\n",
        "    classes.append(12)\n",
        "   \n",
        "\n",
        "filenames = glob.glob(results_dir + 'train-013'+'*.jpg')\n",
        "\n",
        "for j in range(len(filenames)):      \n",
        "    \n",
        "    im_gbr = cv2.imread(filenames[j])\n",
        "    name = os.path.splitext(os.path.basename(filenames[j]))[0]\n",
        "    im = cv2.cvtColor(im_gbr,cv2.COLOR_BGR2RGB)                                   # convert from BGR ordering to RGB\n",
        "\n",
        "    images.append(im)\n",
        "    names.append(name)\n",
        "    classes.append(13)\n",
        "\n",
        "filenames = glob.glob(results_dir + 'train-014'+'*.jpg')\n",
        "\n",
        "for j in range(len(filenames)):      \n",
        "    \n",
        "    im = cv2.imread(filenames[j])\n",
        "    name = os.path.splitext(os.path.basename(filenames[j]))[0]\n",
        "    im = cv2.cvtColor(im_gbr,cv2.COLOR_BGR2RGB)                                    # convert from BGR ordering to RGB\n",
        "\n",
        "    images.append(im)\n",
        "    names.append(name)\n",
        "    classes.append(14)\n",
        "   \n",
        "print('read ', len(images), ' images ')    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oygZTsu97Jr"
      },
      "outputs": [],
      "source": [
        "(set(classes))                                                                       #different train sets classes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmQpzwPzIfbt"
      },
      "source": [
        "Plotting random images from their respective classes of training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BSt9vGhzXgG"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "for i in range(32):\n",
        "\n",
        "    r = np.random.randint(0,len(images))\n",
        "    \n",
        "    plt.subplot(4,8,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.title('train set = '+str(classes[r]))\n",
        "    plt.imshow(images[r])\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZscYRwJI-Xh"
      },
      "source": [
        "**Finding SIFT features**: After importing the cards, to find features of card, SIFT(Scale-invariant feature transform) features have been used. SIFT features are invariant to scale and rotation. Opencv provides sift_create() to get image features.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeHoOXIk-Q1H"
      },
      "outputs": [],
      "source": [
        "\n",
        "sift = cv2.SIFT_create(contrastThreshold=0.02,sigma=1.35)                         # sift engine\n",
        "\n",
        "print('Finding sift features over all the images...')\n",
        "\n",
        "kps = []\n",
        "keypoint_img=[]                                                                   # for keypoints\n",
        "descr_im=[]                                                                       # for descriptors\n",
        "for i in range(len(images)):                                                      # over all the images\n",
        "    \n",
        "    image = images[i]\n",
        "                                                     \n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)                                # calculate SIFT features on gray-scale version of image\n",
        "    kp, desc = sift.detectAndCompute(gray, None)                                  # computing descriptors and keypoints  \n",
        "    \n",
        "    assert len(kp)>0\n",
        "    \n",
        "    kps.append(len(kp))                                                           \n",
        "    keypoint_img.append(kp)\n",
        "    descr_im.append(desc)\n",
        "\n",
        "print('...done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfQ3BHkPF_co"
      },
      "outputs": [],
      "source": [
        "#plotting keypoints for a random image from training set\n",
        "\n",
        "image= images[70]\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "img=cv2.drawKeypoints(gray,keypoint_img[0],image,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)          #function to draw keypoints\n",
        "resized_image = cv2.resize(img, (300,300))\n",
        "\n",
        "cv2_imshow(resized_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bSiC_skNtmX"
      },
      "outputs": [],
      "source": [
        "# We can plot a histogram of the numbers of key points which were \n",
        "# extracted across all the images\n",
        "print('Number of images run ', len(kps))\n",
        "\n",
        "# histogram of numbers of kps \n",
        "plt.hist(np.array(kps), bins=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2-5ZMujJwsP"
      },
      "source": [
        "**Implementing feature matcher using Flann Matcher on pair of images**: Flann feature matcher works on pair of features of images. It tries to find the match between the descriptors obtained from SIFT features. Using Lowe's distance test between the matches, maximum matches are returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-V0A17g1Q8H"
      },
      "outputs": [],
      "source": [
        "FLANN_INDEX_KDTREE = 0                                                           #Flann KD Tree algorithm \n",
        "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)                   #index params with trees set to 5 \n",
        "search_params = dict(checks=5)                                                   #search params with checks set to 5     \n",
        "flann = cv2.FlannBasedMatcher(index_params,search_params)                        #Flann Matcher initialized with index and search params "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpCmYXGO1BUJ"
      },
      "outputs": [],
      "source": [
        "def flannmatch(id1, id2):                                                         #id1 and id2 represents the index of the images for \n",
        "                                                                                  #which feature matching will be performed   \n",
        "  matches = flann.knnMatch(descr_im[id1],descr_im[id2],k=2)                       #flann matching on descriptors for images with index id1 and id2\n",
        "  maxCount=0\n",
        "  count = 0\n",
        "                                                                                  # ratio test as per Lowe's paper\n",
        "  for (m,n) in matches:\n",
        "    if m.distance < 0.7 * n.distance:                                             #matches which satisfy the distance condition are stored     \n",
        "          count += 1      \n",
        "\n",
        "  if count > maxCount:                                                            \n",
        "    maxCount = count                                                              #maxcount of matches between images is returned\n",
        "    \n",
        "  return maxCount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jucWj_gLLdYc"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.imshow(images[21])\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(images[76])\n",
        "\n",
        "m_score= flannmatch(21,76)                                                        #finding match for image with index 21 and index 76.\n",
        "print(f'Total number of matches between image 21 and image 76 is {m_score}') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding top 5 matches for train-001 card images**: To implement this, images from train-001 which are in class=1 are compared with images in all other classes. Flann matcher returns score for all the matches which are stored in m_score dictionary with {index, score} key-value pairs. The scores are then sorted and top 5 images with highest scores are filtered. match_list_001 contains the top 5 scoring card indexes for all 16 images from train_001.    \n"
      ],
      "metadata": {
        "id": "TGdLZ4q3QyHI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-FeAWnZ-Bar"
      },
      "outputs": [],
      "source": [
        "match_list_001=[]\n",
        "for i in range(len(classes)):\n",
        "    m_score={}\n",
        "    if classes[i]==1:\n",
        "      print(f'finding match for {i+1} image')\n",
        "    for j in range(len(classes)):\n",
        "      if classes[i]==1:\n",
        "        if classes[j]!=1:\n",
        "          k=flannmatch(i,j)\n",
        "          m_score[j]= k\n",
        "    li=sorted(m_score, key=m_score.get, reverse=True)[:5]\n",
        "\n",
        "    match_list_001.append(li)                                                    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The 16 \"unidentified\" images from train-001 set are shown below**: These are the images from class=1 for which we ran the above process of feature matching and found top-5 scoring card matches.  "
      ],
      "metadata": {
        "id": "jDFDEw7EUCYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qhn42zrZ3lk"
      },
      "outputs": [],
      "source": [
        "#plotting images of train-001 set.\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(len(classes)):\n",
        "  if classes[i]==1:                                                               #class=1                                             \n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'image {i+1}:{names[i]}')\n",
        "    plt.imshow(images[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-5 matches found** : The top-5 matching cards are found for each train-001 image. For Image 1, matches like Dr Constantine, Slitheen, Mr Crant, Surgeon and Rodrick are found which are all from different training set images. Similarly for other 15 images, top-5 matches can be seen below. \n",
        "Since there are some duplicate images in training sets, the feature matcher will return those as well as it is iterating on all images irrespective of IDs."
      ],
      "metadata": {
        "id": "k2xUIHz2V07k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import gridspec                                                       #to create grid of images\n",
        "nrow = 16                             \n",
        "ncol = 5\n",
        "\n",
        "fig = plt.figure(figsize=(40, 40))    \n",
        "\n",
        "gs = gridspec.GridSpec(nrow, ncol, width_ratios=[0.5, 0.5, 0.5,0.5,0.5],             \n",
        "         wspace=0.5, hspace=0.5, top=0.95, bottom=0.05, left=0.17, right=0.845) \n",
        "\n",
        "for i in range(len(match_list_001)):                                                  #match_list_001 is iterated\n",
        "    for j in range(len(match_list_001[i])):\n",
        "      r= match_list_001[i][j]                                                         #match_list_001[i][j] contains the index of the matched image\n",
        "      im = images[r]                                                                  #image with r index is found from images list\n",
        "      ax= plt.subplot(gs[i,j])                                                        #subplots being created\n",
        "      ax.imshow(im)\n",
        "      ax.set_xticklabels([])\n",
        "      ax.set_yticklabels([])\n",
        "      ax.title.set_text(f'image {i+1} match is: {names[r]}')               \n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "50R1uDYT3g25"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "task-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}